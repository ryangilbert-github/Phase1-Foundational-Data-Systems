# Phase 1: Foundational Data Systems and Backend Architecture

This repository holds the foundational projects in Python, Data Science, and Full-Stack development, demonstrating core competency in data wrangling, algorithmic efficiency, and scalable application architecture. These skills are essential prerequisites for transitioning into specialized Generative AI Engineering roles.

All projects utilize clean code and focus on the principles of efficiency and modular design.

## Project Structure

|Folder|Project Name|Key Skills Demonstrated|
|---|---|---|
|`esports-winrate-engine/`|Python Capstone: E-Sports Prediction|Supervised Learning, Feature Engineering, EDA|
|`game-economy-simulator/`|Python Capstone: Economy Simulation|Computational Complexity, OOP, Graph Algorithms|
|`full_stack_mern/`|Full-Stack Application: MERN & RESTful API|RESTful API Architecture, Secure Software Design, NoSQL|
|`data_dashboard/`|Interactive Data Visualization Dashboard|Data Wrangling, Statistical Visualization, Streamlit|

## Project 1: E-Sports Win-Rate Engine & Game Economy Simulator

_See the respective Python files in the folders above for code logic._

This section contains code focused on using Python for data analysis and simulation.

## Project 2: Full-Stack Application: MERN & RESTful API Architecture

This project is a critical precursor to deploying Generative AI models, demonstrating the ability to build a secure, scalable back-end that can serve API requests efficiently. This is the exact structure needed to deploy Python-based LLM services (like Flask) which then communicate with a front-end interface (like React).

## Project 3: Interactive Data Visualization Dashboard

This project demonstrates the **Exploratory Data Analysis (EDA)** and visualization pipeline crucial for any Generative AI or Machine Learning workflow. The Streamlit dashboard showcases proficiency in cleaning complex data (**Data Wrangling**), performing **Statistical Visualization** (using Matplotlib and Seaborn) to uncover biases and trends, and validating data integrity _before_ it is fed into a training model. It verifies my ability to prepare the high-quality data necessary for successful LLM Fine-Tuning and feature engineering.
